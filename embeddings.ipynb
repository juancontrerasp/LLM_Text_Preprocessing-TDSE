{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Text Preprocessing Foundations\n",
    "\n",
    "This notebook explores the fundamental concepts from Chapter 2 of *Build a Large Language Model (From Scratch)* by Sebastian Raschka.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand tokenization strategies (word-level, character-level, subword)\n",
    "- Implement Byte Pair Encoding (BPE) tokenization\n",
    "- Create training samples using sliding windows\n",
    "- Generate token embeddings\n",
    "- Experiment with hyperparameters and understand their impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.10.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Text Data\n",
    "\n",
    "The quality and preprocessing of training data directly impacts model performance. For LLMs and agentic systems:\n",
    "\n",
    "- **Data is the foundation**: Models learn patterns, syntax, semantics, and even reasoning from raw text\n",
    "- **Preprocessing choices matter**: How we clean and structure text affects what the model learns\n",
    "- **Scale requirements**: LLMs need massive text corpora (billions of tokens) to learn language effectively\n",
    "- **Agentic implications**: For AI agents to interact naturally, they must be trained on diverse, high-quality conversational and instructional text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "First 500 characters:\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Total characters: {len(raw_text)}\")\n",
    "print(f\"First 500 characters:\\n{raw_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is the critical bridge between human language and machine learning\n",
    "\n",
    "- **Vocabulary size tradeoff**: \n",
    "  - Word-level: Large vocabularies (100K+ words), but handles known words well\n",
    "  - Character-level: Tiny vocabulary (~100), but sequences become very long\n",
    "\n",
    "- **Why BPE wins for LLMs**:\n",
    "  - Efficiently handles rare words by breaking them into common subwords\n",
    "  - No \"unknown token\" problem for new words\n",
    "  - Balances sequence length with vocabulary size\n",
    "\n",
    "- **Impact on agentic systems**:\n",
    "  - Better tokenization = better understanding of domain-specific terms, code, URLs, etc.\n",
    "  - Agents need to handle diverse inputs (technical terms, names, multilingual text)\n",
    "  - Token efficiency directly affects inference cost and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens (simple): 4649\n",
      "First 30 tokens: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Simple word-level tokenization (baseline)\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f\"Total tokens (simple): {len(preprocessed)}\")\n",
    "print(f\"First 30 tokens: {preprocessed[:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1159\n",
      "First 20 vocabulary entries: [('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9), ('?', 10), ('A', 11), ('Ah', 12), ('Among', 13), ('And', 14), ('Are', 15), ('Arrt', 16), ('As', 17), ('At', 18), ('Be', 19)]\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "print(f\"First 20 vocabulary entries: {list(vocab.items())[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "Decoded: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer class\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Encoded: {ids}\")\n",
    "print(f\"Decoded: {tokenizer.decode(ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Decoded text: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n",
      "\n",
      "GPT-2 vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Use GPT-2's BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(f\"Encoded text: {integers}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(integers)}\")\n",
    "print(f\"\\nGPT-2 vocabulary size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in text: 5145\n",
      "First 50 token IDs: [40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the full text\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Total tokens in text: {len(enc_text)}\")\n",
    "print(f\"First 50 token IDs: {enc_text[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with Sliding Windows\n",
    "\n",
    "LLMs learn to predict the next token given a context window. The sliding window approach is crucial:\n",
    "\n",
    "- **Context learning**: Each sample teaches the model to predict token N+1 given tokens 1 to N\n",
    "- **Efficient training**: By using overlapping windows (stride < max_length), we create more training examples from limited data\n",
    "- **Position awareness**: Models learn that tokens at different positions have different roles\n",
    "- **Sequence modeling**: This structure teaches causality—the model learns that earlier tokens influence later ones\n",
    "\n",
    "**For agentic systems**: \n",
    "- Sliding windows help models learn multi-turn conversations\n",
    "- Overlapping contexts improve understanding of long-range dependencies\n",
    "- This is why agents can maintain coherent conversations—they learn from overlapping context windows during training\n",
    "\n",
    "### The `max_length` and `stride` Parameters:\n",
    "- **max_length**: The context window size (how many tokens the model sees)\n",
    "- **stride**: How many tokens to shift the window (smaller stride = more overlap = more training samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader for creating input-target pairs\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "class GPTDatasetV1(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 4])\n",
      "Target shape: torch.Size([8, 4])\n",
      "\n",
      "Input batch:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Target batch:\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# Create a dataloader with default parameters\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print(f\"\\nInput batch:\\n{inputs}\")\n",
    "print(f\"\\nTarget batch:\\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token Embeddings\n",
    "\n",
    "Embeddings turn words (tokens) into numbers the model can work with.  \n",
    "Instead of `\"cat\" = ID 2435`, it becomes a vector like `[0.23, -0.45, 0.12, ...]`.  \n",
    "That vector is learned, not hand-coded.\n",
    "\n",
    "### Why do embeddings capture meaning?\n",
    "\n",
    "- **Similar words end up close together**  \n",
    "  Words used in similar contexts get similar vectors  \n",
    "  (`cat ≈ dog`, measured with cosine similarity)\n",
    "\n",
    "- **They’re learned during training**  \n",
    "  The model starts with random vectors  \n",
    "  Backpropagation nudges them so words that appear together move closer  \n",
    "  Over time, meaning emerges from usage\n",
    "\n",
    "- **Meaning becomes geometry**  \n",
    "  Position = meaning  \n",
    "  Relationships become directions  \n",
    "  (`king - man + woman ≈ queen`)\n",
    "\n",
    "### How embeddings fit into neural networks\n",
    "\n",
    "- **They’re just a learnable matrix**  \n",
    "  Token ID → row lookup → dense vector  \n",
    "  Simple, but powerful\n",
    "\n",
    "- **They’re the first layer**  \n",
    "  Everything else (attention, MLPs) builds on embeddings  \n",
    "  Bad embeddings = weak understanding\n",
    "\n",
    "- **They’re shared everywhere**  \n",
    "  The word `\"cat\"` uses the same base embedding no matter where it appears  \n",
    "  Classic parameter sharing\n",
    "\n",
    "- **Size matters**  \n",
    "  Small vectors: faster, less expressive  \n",
    "  Large vectors: richer meaning, more compute  \n",
    "  (bias–variance tradeoff)\n",
    "\n",
    "### Why this matters for agents\n",
    "\n",
    "- Understands synonyms (`buy ≈ purchase`)\n",
    "- Transfers knowledge to new tasks\n",
    "- Context refines meaning (`bank` ≠ `river bank`)\n",
    "\n",
    "**Bottom line:**  \n",
    "Embeddings turn language into a continuous space where learning works.  \n",
    "LLMs don’t “understand” words directly — they understand *where words live* relative to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer shape: torch.Size([50257, 256])\n",
      "This is a matrix with 50,257 rows (one per token) and 256 columns (embedding dimensions)\n"
     ]
    }
   ],
   "source": [
    "# Create token embeddings\n",
    "vocab_size = 50257  # GPT-2's vocabulary size\n",
    "output_dim = 256    # Embedding dimension\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(f\"Embedding layer shape: {embedding_layer.weight.shape}\")\n",
    "print(f\"This is a matrix with {vocab_size:,} rows (one per token) and {output_dim} columns (embedding dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs: tensor([   2,    5, 1000, 8000])\n",
      "Embedded vectors shape: torch.Size([4, 256])\n",
      "\n",
      "First embedded vector (token ID 2):\n",
      "tensor([-1.3152, -0.0677, -0.1350, -0.5183,  0.2326,  1.6311, -0.8707,  0.0490,\n",
      "        -0.2526,  0.0810,  1.2702,  0.7397,  1.0890, -0.6591, -0.1910,  0.5061,\n",
      "         1.7788, -0.7092, -1.5262, -1.4693], grad_fn=<SliceBackward0>)...\n"
     ]
    }
   ],
   "source": [
    "# Example: embed some token IDs\n",
    "sample_ids = torch.tensor([2, 5, 1000, 8000])\n",
    "embedded = embedding_layer(sample_ids)\n",
    "\n",
    "print(f\"Input token IDs: {sample_ids}\")\n",
    "print(f\"Embedded vectors shape: {embedded.shape}\")\n",
    "print(f\"\\nFirst embedded vector (token ID 2):\\n{embedded[0][:20]}...\")  # Show first 20 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([8, 4])\n",
      "Token embeddings shape: torch.Size([8, 4, 256])\n",
      "Each batch item now has 4 tokens, each represented by a 256-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Embed a batch from our dataloader\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "token_embeddings = embedding_layer(inputs)\n",
    "print(f\"Input batch shape: {inputs.shape}\")\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "print(f\"Each batch item now has {token_embeddings.shape[1]} tokens, each represented by a {token_embeddings.shape[2]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings shape: torch.Size([4, 256])\n",
      "Each position (0 to 3) has its own 256-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Positional embeddings\n",
    "context_length = 4  # Same as max_length in our dataloader\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(f\"Positional embeddings shape: {pos_embeddings.shape}\")\n",
    "print(f\"Each position (0 to {context_length-1}) has its own {output_dim}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input embeddings shape: torch.Size([8, 4, 256])\n",
      "\n",
      "These combined embeddings are what feed into the transformer blocks!\n",
      "Each token now has both semantic (what it means) and positional (where it is) information.\n"
     ]
    }
   ],
   "source": [
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\"Final input embeddings shape: {input_embeddings.shape}\")\n",
    "print(f\"\\nThese combined embeddings are what feed into the transformer blocks!\")\n",
    "print(f\"Each token now has both semantic (what it means) and positional (where it is) information.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
